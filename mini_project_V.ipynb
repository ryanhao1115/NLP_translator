{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "mini_project_V.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B0-7x4L4aVz"
      },
      "source": [
        "## Language Translator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3FLPFbU4aV4"
      },
      "source": [
        "import nltk\n",
        "import collections\n",
        "#import helper\n",
        "import numpy as np\n",
        "#import project_tests as tests\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZEyrJRG5IYZ"
      },
      "source": [
        "import joblib\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbaubNYQ4aV5"
      },
      "source": [
        "From `nltk` we can download translated sentences between different languages. You can see the example between **English and French** below but feel free to try different combination as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_JpfjSv4aV6",
        "outputId": "e5f76543-24e1-4384-f89e-e12d5b77578a"
      },
      "source": [
        "nltk.download('comtrans')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package comtrans to\n",
            "[nltk_data]     C:\\Users\\huawei\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oXxkDTL4aV7",
        "outputId": "f320bd01-7bed-4191-dad2-ed72e896de6f"
      },
      "source": [
        "from nltk.corpus import comtrans\n",
        "print(comtrans.aligned_sents('alignment-en-fr.txt')[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<AlignedSent: 'Resumption of the se...' -> 'Reprise de la sessio...'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nlm9ilP4aV7",
        "outputId": "4e1e292f-a508-4b11-8c1a-25e062c23927"
      },
      "source": [
        "len(comtrans.aligned_sents('alignment-en-fr.txt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33334"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwjWrdBS4aV_",
        "outputId": "8da31a7c-108d-47f0-b953-5365a0d63313"
      },
      "source": [
        "eng_text = []\n",
        "frn_text = []\n",
        "for i in range(334):\n",
        "    als = comtrans.aligned_sents(\"alignment-en-fr.txt\")[i]\n",
        "    eng_text.append(\" \".join(als.words))\n",
        "    frn_text.append(\" \".join(als.mots))\n",
        "print(len(eng_text))\n",
        "print(len(frn_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334\n",
            "334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqSeb1wF47Wc"
      },
      "source": [
        "frn_text = joblib.load('frn_text.joblib')\n",
        "eng_text = joblib.load('eng_text.joblib')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8qwmLkLeA3Mc",
        "outputId": "69462c77-ab76-4cb0-a06f-253c2ed142e1"
      },
      "source": [
        "eng_text[18]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is all in accordance with the principles that we have always upheld .'"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR7JCj2m4aWB",
        "outputId": "d2e0c26e-8c60-42f1-bca6-21b699aed17e"
      },
      "source": [
        "## Tokenzie\n",
        "def tokenize(x):\n",
        "    x_tk = Tokenizer(char_level = False)\n",
        "    x_tk.fit_on_texts(x)\n",
        "    return x_tk.texts_to_sequences(x), x_tk\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  The quick brown fox jumps over the lazy dog .\n",
            "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
            "Sequence 2 in x\n",
            "  Input:  By Jove , my quick study of lexicography won a prize .\n",
            "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
            "Sequence 3 in x\n",
            "  Input:  This is a short sentence .\n",
            "  Output: [18, 19, 3, 20, 21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "radfwJYw4aWC"
      },
      "source": [
        "##padding\n",
        "def pad(x, length=None):\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    return pad_sequences(x, maxlen = length, padding = 'post')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsmdfiu64aWC"
      },
      "source": [
        "## combine preparcessing\n",
        "def preprocess(x, y):\n",
        "    preprocess_x, x_tk = tokenize(eng_text)\n",
        "    preprocess_y, y_tk = tokenize(frn_text)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "    \n",
        "    #preprocess_x = preprocess_x.reshape(*preprocess_x.shape, 1)\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "    \n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfgZ-Wtt4aWD",
        "outputId": "90c6eb20-352e-49bd-e92a-342b81a6c5ed"
      },
      "source": [
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(eng_text, frn_text)\n",
        "    \n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 41\n",
            "Max French sentence length: 41\n",
            "English vocabulary size: 15553\n",
            "French vocabulary size: 21326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6BLLgP44aWD",
        "outputId": "dca0c168-2bd8-4127-8c78-c70c7d721397"
      },
      "source": [
        "preproc_english_sentences.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31500, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzppT02D4aWE"
      },
      "source": [
        "## from ids to words\n",
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "    print('`logits_to_text` function loaded.')\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bNuK3jM4aWE",
        "outputId": "968b2894-49eb-40c2-f5ad-aa4a5b33aa97"
      },
      "source": [
        "## building models\n",
        "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    input_seq = Input(input_shape[1:])\n",
        "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
        "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
        "    model = Model(input_seq, Activation('softmax')(logits))\n",
        "    model.compile(loss = 'sparse_categorical_crossentropy', \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "#tests.test_simple_model(simple_model)\n",
        "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "\n",
        "# Train the neural network\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size)\n",
        "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=50, epochs=10, validation_split=0.2)\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6/6 [==============================] - 1s 250ms/step - loss: 7.3790 - accuracy: 0.3301 - val_loss: 7.3010 - val_accuracy: 0.3920\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 1s 142ms/step - loss: 7.2434 - accuracy: 0.4830 - val_loss: 7.1744 - val_accuracy: 0.4101\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 1s 180ms/step - loss: 7.0917 - accuracy: 0.4898 - val_loss: 7.0117 - val_accuracy: 0.4108\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 1s 145ms/step - loss: 6.8810 - accuracy: 0.4894 - val_loss: 6.7507 - val_accuracy: 0.4081\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 1s 140ms/step - loss: 6.4844 - accuracy: 0.4904 - val_loss: 6.2135 - val_accuracy: 0.4124\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 1s 132ms/step - loss: 5.7724 - accuracy: 0.4925 - val_loss: 5.6903 - val_accuracy: 0.4179\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 1s 173ms/step - loss: 5.2103 - accuracy: 0.4976 - val_loss: 5.2998 - val_accuracy: 0.4226\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 1s 150ms/step - loss: 4.7604 - accuracy: 0.5015 - val_loss: 4.9493 - val_accuracy: 0.4242\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 1s 123ms/step - loss: 4.3586 - accuracy: 0.5057 - val_loss: 4.6458 - val_accuracy: 0.4254\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 1s 135ms/step - loss: 4.0082 - accuracy: 0.5062 - val_loss: 4.4106 - val_accuracy: 0.4277\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000215A3B53310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "`logits_to_text` function loaded.\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkSE0f1j4aWF",
        "outputId": "9a01e86d-083e-4f2d-8047-db153a0d01e1"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    learning_rate = 1e-3\n",
        "    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n",
        "    \n",
        "    embedding = Embedding(french_vocab_size, 64, input_length=input_shape[1]) \n",
        "    logits = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"))\n",
        "    \n",
        "    model = Sequential()\n",
        "    #em can only be used in first layer --> Keras Documentation\n",
        "    model.add(embedding)\n",
        "    model.add(rnn)\n",
        "    model.add(logits)\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "#tests.test_embed_model(embed_model)\n",
        "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
        "embeded_model = embed_model(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size)\n",
        "embeded_model.fit(tmp_x, preproc_french_sentences, batch_size=50, epochs=10, validation_split=0.2)\n",
        "print(logits_to_text(embeded_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6/6 [==============================] - 1s 218ms/step - loss: 7.3983 - accuracy: 0.3123 - val_loss: 7.3751 - val_accuracy: 0.4273\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 7.3425 - accuracy: 0.5060 - val_loss: 7.3043 - val_accuracy: 0.4277\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 7.2271 - accuracy: 0.5059 - val_loss: 7.1303 - val_accuracy: 0.4277\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 6.9096 - accuracy: 0.5059 - val_loss: 6.5676 - val_accuracy: 0.4277\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 5.9510 - accuracy: 0.5059 - val_loss: 5.4200 - val_accuracy: 0.4277\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 4.8380 - accuracy: 0.5059 - val_loss: 4.7594 - val_accuracy: 0.4277\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 1s 119ms/step - loss: 4.2139 - accuracy: 0.5059 - val_loss: 4.4302 - val_accuracy: 0.4277\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 1s 166ms/step - loss: 3.8599 - accuracy: 0.5059 - val_loss: 4.3125 - val_accuracy: 0.4277\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 1s 119ms/step - loss: 3.7221 - accuracy: 0.5059 - val_loss: 4.3342 - val_accuracy: 0.4277\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 3.6940 - accuracy: 0.5059 - val_loss: 4.3788 - val_accuracy: 0.4277\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021588F7E0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "`logits_to_text` function loaded.\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "LPpsFMyc4aWG",
        "outputId": "abf0c40d-2b95-4629-ea2a-23db4274fae1"
      },
      "source": [
        "## Model 3\n",
        "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "   \n",
        "    learning_rate = 1e-3\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), \n",
        "                           input_shape = input_shape[1:]))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    return model\n",
        "#tests.test_bd_model(bd_model)\n",
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "bidi_model = bd_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "bidi_model.fit(tmp_x, preproc_french_sentences, batch_size=1000, epochs=20, validation_split=0.2)\n",
        "#joblib.dump(bidi_model,'bidimodel.joblib')\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(bidi_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "26/26 [==============================] - 122s 5s/step - loss: 8.1497 - accuracy: 0.3447 - val_loss: 5.1426 - val_accuracy: 0.5164\n",
            "Epoch 2/20\n",
            "26/26 [==============================] - 115s 4s/step - loss: 4.2973 - accuracy: 0.5068 - val_loss: 3.9316 - val_accuracy: 0.5137\n",
            "Epoch 3/20\n",
            "26/26 [==============================] - 117s 5s/step - loss: 3.8195 - accuracy: 0.5067 - val_loss: 3.6780 - val_accuracy: 0.5137\n",
            "Epoch 4/20\n",
            "26/26 [==============================] - 117s 4s/step - loss: 3.6438 - accuracy: 0.5069 - val_loss: 3.5705 - val_accuracy: 0.5145\n",
            "Epoch 5/20\n",
            "26/26 [==============================] - 116s 4s/step - loss: 3.5645 - accuracy: 0.5087 - val_loss: 3.5278 - val_accuracy: 0.5187\n",
            "Epoch 6/20\n",
            "26/26 [==============================] - 118s 5s/step - loss: 3.5294 - accuracy: 0.5118 - val_loss: 3.5118 - val_accuracy: 0.5225\n",
            "Epoch 7/20\n",
            "26/26 [==============================] - 115s 4s/step - loss: 3.5116 - accuracy: 0.5143 - val_loss: 3.5004 - val_accuracy: 0.5231\n",
            "Epoch 8/20\n",
            "26/26 [==============================] - 115s 4s/step - loss: 3.4982 - accuracy: 0.5155 - val_loss: 3.4933 - val_accuracy: 0.5237\n",
            "Epoch 9/20\n",
            "26/26 [==============================] - 118s 5s/step - loss: 3.4877 - accuracy: 0.5163 - val_loss: 3.4848 - val_accuracy: 0.5244\n",
            "Epoch 10/20\n",
            "26/26 [==============================] - 115s 4s/step - loss: 3.4787 - accuracy: 0.5171 - val_loss: 3.4787 - val_accuracy: 0.5248\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-8181b76f8d9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     len(french_tokenizer.word_index)+1)\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mbidi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc_french_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#joblib.dump(bidi_model,'bidimodel.joblib')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Print prediction(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6UMAE-Ub_Oj",
        "outputId": "7bb7e293-d6ad-4168-b803-c27a554a3c65"
      },
      "source": [
        "print(logits_to_text(bidi_model.predict(tmp_x[:5])[0], french_tokenizer))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`logits_to_text` function loaded.\n",
            "de de <PAD> de <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ_KwbNT4aWI",
        "outputId": "b0d092c3-d492-4fcf-9a5b-1cacc5317db8"
      },
      "source": [
        "## Model4\n",
        "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "  \n",
        "    learning_rate = 1e-3\n",
        "    model = Sequential()\n",
        "    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(GRU(128, return_sequences = True))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
        "    \n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    return model\n",
        "#tests.test_encdec_model(encdec_model)\n",
        "tmp_x = pad(preproc_english_sentences)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1))\n",
        "encodeco_model = encdec_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_french_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "encodeco_model.fit(tmp_x, preproc_french_sentences, batch_size=50, epochs=20, validation_split=0.2)\n",
        "print(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "6/6 [==============================] - 2s 328ms/step - loss: 7.3923 - accuracy: 0.4091 - val_loss: 7.3319 - val_accuracy: 0.4277\n",
            "Epoch 2/20\n",
            "6/6 [==============================] - 1s 156ms/step - loss: 7.1805 - accuracy: 0.5059 - val_loss: 6.5681 - val_accuracy: 0.4277\n",
            "Epoch 3/20\n",
            "6/6 [==============================] - 1s 176ms/step - loss: 5.4024 - accuracy: 0.5059 - val_loss: 4.5671 - val_accuracy: 0.4277\n",
            "Epoch 4/20\n",
            "6/6 [==============================] - 1s 152ms/step - loss: 3.9294 - accuracy: 0.5059 - val_loss: 4.3088 - val_accuracy: 0.4277\n",
            "Epoch 5/20\n",
            "6/6 [==============================] - 1s 193ms/step - loss: 3.6809 - accuracy: 0.5059 - val_loss: 4.3224 - val_accuracy: 0.4277\n",
            "Epoch 6/20\n",
            "6/6 [==============================] - 1s 159ms/step - loss: 3.6264 - accuracy: 0.5059 - val_loss: 4.3232 - val_accuracy: 0.4277\n",
            "Epoch 7/20\n",
            "6/6 [==============================] - 1s 201ms/step - loss: 3.5956 - accuracy: 0.5059 - val_loss: 4.3104 - val_accuracy: 0.4277\n",
            "Epoch 8/20\n",
            "6/6 [==============================] - 1s 173ms/step - loss: 3.5562 - accuracy: 0.5059 - val_loss: 4.3102 - val_accuracy: 0.4277\n",
            "Epoch 9/20\n",
            "6/6 [==============================] - 1s 153ms/step - loss: 3.5233 - accuracy: 0.5059 - val_loss: 4.2945 - val_accuracy: 0.4277\n",
            "Epoch 10/20\n",
            "6/6 [==============================] - 1s 158ms/step - loss: 3.4999 - accuracy: 0.5059 - val_loss: 4.2944 - val_accuracy: 0.4277\n",
            "Epoch 11/20\n",
            "6/6 [==============================] - 1s 174ms/step - loss: 3.4789 - accuracy: 0.5059 - val_loss: 4.2943 - val_accuracy: 0.4277\n",
            "Epoch 12/20\n",
            "6/6 [==============================] - 1s 156ms/step - loss: 3.4604 - accuracy: 0.5059 - val_loss: 4.2846 - val_accuracy: 0.4277\n",
            "Epoch 13/20\n",
            "6/6 [==============================] - 1s 152ms/step - loss: 3.4472 - accuracy: 0.5059 - val_loss: 4.2807 - val_accuracy: 0.4277\n",
            "Epoch 14/20\n",
            "6/6 [==============================] - 1s 156ms/step - loss: 3.4285 - accuracy: 0.5059 - val_loss: 4.2741 - val_accuracy: 0.4277\n",
            "Epoch 15/20\n",
            "6/6 [==============================] - 1s 181ms/step - loss: 3.4120 - accuracy: 0.5059 - val_loss: 4.2881 - val_accuracy: 0.4277\n",
            "Epoch 16/20\n",
            "6/6 [==============================] - 1s 165ms/step - loss: 3.3973 - accuracy: 0.5059 - val_loss: 4.2681 - val_accuracy: 0.4277\n",
            "Epoch 17/20\n",
            "6/6 [==============================] - 1s 148ms/step - loss: 3.3836 - accuracy: 0.5059 - val_loss: 4.2655 - val_accuracy: 0.4277\n",
            "Epoch 18/20\n",
            "6/6 [==============================] - 1s 157ms/step - loss: 3.3745 - accuracy: 0.5059 - val_loss: 4.3069 - val_accuracy: 0.4277\n",
            "Epoch 19/20\n",
            "6/6 [==============================] - 1s 208ms/step - loss: 3.3670 - accuracy: 0.5059 - val_loss: 4.2414 - val_accuracy: 0.4277\n",
            "Epoch 20/20\n",
            "6/6 [==============================] - 1s 197ms/step - loss: 3.3526 - accuracy: 0.5059 - val_loss: 4.2836 - val_accuracy: 0.4277\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002159BD1EDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "`logits_to_text` function loaded.\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u-vw42m4aWI",
        "outputId": "8915881d-b7dd-4de2-bb91-4659306e6260"
      },
      "source": [
        "## Model 5 combine 3 and 4\n",
        "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "  \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n",
        "    model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "#tests.test_model_final(model_final)\n",
        "print('Final Model Loaded')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jw0S3BanpLs",
        "outputId": "85b66fc8-7e5e-4d6f-c828-37bf3e20cc9f"
      },
      "source": [
        "#def final_predictions(x, y, x_tk, y_tk):\n",
        "    \n",
        "tmp_X = pad(preproc_english_sentences)\n",
        "model = model_final(tmp_X.shape,\n",
        "                        preproc_french_sentences.shape[1],\n",
        "                        len(english_tokenizer.word_index)+1,\n",
        "                        len(french_tokenizer.word_index)+1)\n",
        "    \n",
        "model.fit(tmp_X, preproc_french_sentences, batch_size = 500, epochs = 18, validation_split = 0.2)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/18\n",
            "51/51 [==============================] - 196s 4s/step - loss: 4.3432 - accuracy: 0.4967 - val_loss: 3.7998 - val_accuracy: 0.5180\n",
            "Epoch 2/18\n",
            "51/51 [==============================] - 190s 4s/step - loss: 3.7337 - accuracy: 0.5142 - val_loss: 3.6736 - val_accuracy: 0.5281\n",
            "Epoch 3/18\n",
            "51/51 [==============================] - 189s 4s/step - loss: 3.6565 - accuracy: 0.5202 - val_loss: 3.6996 - val_accuracy: 0.5273\n",
            "Epoch 4/18\n",
            "51/51 [==============================] - 187s 4s/step - loss: 3.6408 - accuracy: 0.5215 - val_loss: 3.6345 - val_accuracy: 0.5276\n",
            "Epoch 5/18\n",
            "51/51 [==============================] - 191s 4s/step - loss: 3.6212 - accuracy: 0.5224 - val_loss: 3.6349 - val_accuracy: 0.5284\n",
            "Epoch 6/18\n",
            "51/51 [==============================] - 190s 4s/step - loss: 3.6170 - accuracy: 0.5230 - val_loss: 3.6373 - val_accuracy: 0.5292\n",
            "Epoch 7/18\n",
            "51/51 [==============================] - 191s 4s/step - loss: 3.5962 - accuracy: 0.5242 - val_loss: 3.6537 - val_accuracy: 0.5291\n",
            "Epoch 8/18\n",
            "51/51 [==============================] - 187s 4s/step - loss: 3.5942 - accuracy: 0.5239 - val_loss: 3.6353 - val_accuracy: 0.5317\n",
            "Epoch 9/18\n",
            "51/51 [==============================] - 185s 4s/step - loss: 3.5806 - accuracy: 0.5253 - val_loss: 3.6355 - val_accuracy: 0.5281\n",
            "Epoch 10/18\n",
            "51/51 [==============================] - 188s 4s/step - loss: 3.5546 - accuracy: 0.5261 - val_loss: 3.6192 - val_accuracy: 0.5296\n",
            "Epoch 11/18\n",
            "51/51 [==============================] - 189s 4s/step - loss: 3.5603 - accuracy: 0.5260 - val_loss: 3.6164 - val_accuracy: 0.5294\n",
            "Epoch 12/18\n",
            "51/51 [==============================] - 186s 4s/step - loss: 3.5341 - accuracy: 0.5276 - val_loss: 3.6097 - val_accuracy: 0.5287\n",
            "Epoch 13/18\n",
            "51/51 [==============================] - 186s 4s/step - loss: 3.5322 - accuracy: 0.5276 - val_loss: 3.6250 - val_accuracy: 0.5291\n",
            "Epoch 14/18\n",
            "51/51 [==============================] - 186s 4s/step - loss: 3.5081 - accuracy: 0.5295 - val_loss: 3.6308 - val_accuracy: 0.5252\n",
            "Epoch 15/18\n",
            "51/51 [==============================] - 186s 4s/step - loss: 3.5037 - accuracy: 0.5302 - val_loss: 3.6306 - val_accuracy: 0.5329\n",
            "Epoch 16/18\n",
            "51/51 [==============================] - 188s 4s/step - loss: 3.5027 - accuracy: 0.5306 - val_loss: 3.5979 - val_accuracy: 0.5328\n",
            "Epoch 17/18\n",
            "51/51 [==============================] - 185s 4s/step - loss: 3.4848 - accuracy: 0.5321 - val_loss: 3.6194 - val_accuracy: 0.5344\n",
            "Epoch 18/18\n",
            "51/51 [==============================] - 185s 4s/step - loss: 3.4759 - accuracy: 0.5328 - val_loss: 3.6065 - val_accuracy: 0.5315\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd47de32ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMaKQAO_BIjP",
        "outputId": "8529511f-68f7-46b7-ded2-3abdf4dfc6f9"
      },
      "source": [
        "model.fit(tmp_X, preproc_french_sentences, batch_size = 64, epochs = 3, validation_split = 0.2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "394/394 [==============================] - 359s 912ms/step - loss: 3.8675 - accuracy: 0.5331 - val_loss: 3.9895 - val_accuracy: 0.5366\n",
            "Epoch 2/3\n",
            "394/394 [==============================] - 348s 883ms/step - loss: 3.8906 - accuracy: 0.5355 - val_loss: 3.9832 - val_accuracy: 0.5381\n",
            "Epoch 3/3\n",
            "394/394 [==============================] - 337s 857ms/step - loss: 3.8609 - accuracy: 0.5371 - val_loss: 3.9913 - val_accuracy: 0.5349\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd499275a90>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "m_r5csdE4aWI",
        "outputId": "cf314e18-c45c-4bff-c184-59d553cfce91"
      },
      "source": [
        "\n",
        " \n",
        "y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
        "y_id_to_word[0] = '<PAD>'\n",
        "sentence = ' vote for the   is like a vote for the Liberals'\n",
        "sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
        "sentence = pad_sequences([sentence], maxlen=preproc_english_sentences.shape[-1], padding='post')\n",
        "sentences = np.array([sentence[0], preproc_english_sentences[0]])\n",
        "predictions = model.predict(sentences, len(sentences))\n",
        "print('Sample 1:')\n",
        "print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
        "print('Il a vu un vieux camion jaune')\n",
        "print('Sample 2:')\n",
        "print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
        "print(' '.join([y_id_to_word[np.max(x)] for x in preproc_french_sentences[0]]))\n",
        "\n",
        "    \n",
        "#final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-1fcdc54db56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_id_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' vote for the   is like a vote for the Liberals'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menglish_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreproc_english_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc_english_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-1fcdc54db56c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_id_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' vote for the   is like a vote for the Liberals'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menglish_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreproc_english_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc_english_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Liberals'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC0hQ7PD4aWJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}